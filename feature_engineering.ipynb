{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_dataset import load_dataset\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PowerTransformer, KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "rnd.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(df:pd.DataFrame):\n",
    "\n",
    "    # From PassengerId get GroupId, PassengerId inside the group and Group_size\n",
    "    df[['GroupId', 'PassengerId_no_group']] = df['PassengerId'].str.split('_', expand=True)\n",
    "    df = pd.merge(df, df.groupby('GroupId')['PassengerId'].count(), how='left', on='GroupId')\n",
    "    # Rename columns, that they are more meaningful\n",
    "    df.rename({'PassengerId_y': 'Group_size', 'PassengerId_x': 'Full_Id', 'PassengerId_no_group': 'PassengerId'}, axis=1, inplace=True)\n",
    "    df['PassengerId'] = df['PassengerId'].astype('int64')\n",
    "    df['GroupId'] = df['GroupId'].astype('int64')\n",
    "    df['Group_size'] = df['Group_size'].astype('int64')\n",
    "\n",
    "\n",
    "    # Add cabinmates as column\n",
    "    df = pd.merge(df, df.groupby('Cabin')['PassengerId'].count().rename('Number_of_cabinmates'), how='left', on='Cabin')\n",
    "\n",
    "    # From cabin get Deck, Side and cabin number on deck\n",
    "    df[['Deck', 'Num', 'Side']] = df['Cabin'].str.split('/', expand=True)\n",
    "    df['Deck'] = df['Deck'].astype('category')\n",
    "    df['Side'] = df['Side'].astype('category')\n",
    "    df['Num'] = df['Num'].astype('float64')         # Should be int, but at this stage still NaNs in data, so use float\n",
    "\n",
    "\n",
    "    df['HomePlanet'] = df['HomePlanet'].astype('category')\n",
    "    df['Destination'] = df['Destination'].astype('category')\n",
    "\n",
    "    # Split Name for First Name and Last Name\n",
    "    df[['First_name', 'Last_name']] = df['Name'].str.split(' ', expand=True)\n",
    "\n",
    "    # Last name as category\n",
    "    df['Last_name'] = df['Last_name'].astype('category')\n",
    "    # Add namesakes number as column\n",
    "    df = pd.merge(df, df.groupby('Last_name')['PassengerId'].count().rename('Number_of_namesakes'), how='left', on='Last_name')\n",
    "\n",
    "    df['Number_of_namesakes'] = df['Number_of_namesakes'].astype('float64') # Same reason that for cabin: NaNs are present\n",
    "\n",
    "    \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spending_features(df:pd.DataFrame):\n",
    "    # Fill NaNs with 0 (the most common value)\n",
    "    df[['RoomService', 'FoodCourt','ShoppingMall','Spa','VRDeck']] = df[['RoomService', 'FoodCourt','ShoppingMall','Spa','VRDeck']].fillna(0.0)\n",
    "\n",
    "    # Total spend\n",
    "    df['TotalSpend'] = df['RoomService'] + df['FoodCourt'] + df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n",
    "\n",
    "    # Entertainment spend\n",
    "    df['EntSpend'] = df['ShoppingMall'] + df['Spa'] + df['VRDeck']\n",
    "\n",
    "    # Living spend\n",
    "    df['LivSpend'] = df['RoomService'] + df['FoodCourt']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function, with my modifications, were taken from https://www.kaggle.com/code/ravi20076/sptitanic-bootstrapensemble-pipeline\n",
    "\n",
    "def TrtNullAgeCrSlp(df:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function fills nulls in age and cryosleep as below-\n",
    "    Cryosleep:-\n",
    "    1. For non-spenders, cryosleep=1 as cryosleep customers don't spend\n",
    "    2. If age <=12 and cryosleep is null, then cryosleep= 1\n",
    "    3. For spenders, cryosleep= 0\n",
    "    \n",
    "    Age:-\n",
    "    1. For spenders/ non-cryosleep, median age > 12 for family is considered (child cannot spend)\n",
    "    2. For all remaining nulls, overall median age is used\n",
    "    \n",
    "    Flag for Is_Child (Age <=12) is also created\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Filling nulls in cryosleep based on spending and age details:-\n",
    "    df['CryoSleep'] = np.float16(df['CryoSleep']*1.0)\n",
    "    df.loc[(df.CryoSleep.isna()==True) & (df.TotalSpend == 0.0), ['CryoSleep']] = 1.0\n",
    "    # Assuming child (age <=12) and null cryosleep = cryosleep\n",
    "    df.loc[(df.CryoSleep.isna()==True) & (df.Age <=12), ['CryoSleep']] = 1.0\n",
    "    # Assuming no cryosleep for spenders:-\n",
    "    df.loc[(df.CryoSleep.isna()==True) & (df['TotalSpend'] > 0.0), ['CryoSleep']] = 0.0\n",
    "    df['CryoSleep'] = df['CryoSleep'].astype(np.int8)\n",
    "    \n",
    "    # 2. Assuming average group age for spenders:-\n",
    "    df = df.merge(df.loc[df.Age >12,['GroupId', 'Age']].dropna().groupby('GroupId').agg(_Age= pd.NamedAgg('Age', np.median)), how= 'left', left_on= 'GroupId', right_on='GroupId', suffixes= ('',''))\n",
    "    df.loc[(df.Age.isna()==True) & ((df.TotalSpend > 0.0) | (df.CryoSleep==0)), ['Age']] = df._Age\n",
    "    # Filling median age for remaining nulls:-\n",
    "    df['Age'] = df['Age'].fillna(df.Age.median())\n",
    "    \n",
    "    # 3. Creating flag for child:-\n",
    "    df['Is_Child'] = np.where(df.Age <= 12, 1,0)\n",
    "    df['Is_Child'] = df['Is_Child'].astype(np.int8)\n",
    "    \n",
    "    df = df.drop(['_Age'], axis=1)\n",
    "    df['Age'] = df['Age'].astype(np.int8)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function, with my modifications, were taken from https://www.kaggle.com/code/ravi20076/sptitanic-bootstrapensemble-pipeline\n",
    "def TrtNullVIPCabin(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    This function treats nulls in VIP and cabin columns using the GroupId.\n",
    "    We assume that members of the same family have the same cabin and VIP IDs\n",
    "    As an addition, it downcasts the float64 columns to conserve memory.\n",
    "    \"\"\"\n",
    "    def Extract(lst):\n",
    "        new_lst = []\n",
    "        for item in lst:\n",
    "            if ((type(item) != float) and (len(item)!=0)):\n",
    "                if len(item) > 1:\n",
    "                    new_lst.append(item[0])\n",
    "                else:\n",
    "                    new_lst.append(item)\n",
    "            else:\n",
    "                new_lst.append(np.nan)\n",
    "        return new_lst \n",
    "\n",
    "\n",
    "    # Assuming that members of the same group have the same VIP ID:-\n",
    "    df = df.merge(df[['VIP', 'GroupId']].groupby('GroupId')['VIP'].max(), how = 'left', left_on= 'GroupId', right_on= 'GroupId', suffixes= ('','_'))\n",
    "    df['VIP'] = df['VIP'].fillna(df.VIP_)\n",
    "    df['VIP'] = df['VIP'].fillna(0.0)\n",
    "    df['VIP'] = df['VIP'].astype(np.int8)\n",
    "\n",
    "    # Assuming that members of the same group have the same cabin ID:-\n",
    "    df = df.merge(df[['Num', 'GroupId']].groupby('GroupId')['Num'].max(), how = 'left', left_on= 'GroupId', right_on= 'GroupId', suffixes= ('','_'))\n",
    "    #print(df.VIP_)\n",
    "    df['Num'] = df['Num'].fillna(df.Num_)\n",
    "    df['Num'] = df['Num'].fillna(0.0)\n",
    "    df['Num'] = df['Num'].astype(np.int16)\n",
    "\n",
    "\n",
    "\n",
    "    df = df.merge(df[df['Group_size']>1][['Deck', 'GroupId']].groupby('GroupId')['Deck'].agg(pd.Series.mode), how = 'left', left_on= 'GroupId', right_on= 'GroupId', suffixes= ('','_'))\n",
    "    df['Deck_'] = Extract(list(df['Deck_']))\n",
    "    try:\n",
    "        df['Deck_'] = df['Deck_'].astype('category').cat.add_categories('T')\n",
    "    except ValueError:\n",
    "        pass\n",
    "    # print(df.Deck_.dtype)\n",
    "    df['Deck'] = df['Deck'].fillna(df.Deck_)\n",
    "\n",
    "    # Single passengers are filled with random by the same distribution\n",
    "    categories = (df.Deck.value_counts() / len(df)).index.to_list()\n",
    "    weights = (df.Deck.value_counts() / len(df)).to_list()\n",
    "    rnd_decks = []\n",
    "    for i in range(len(df)):\n",
    "        rnd_decks.append(rnd.choices(categories, weights=weights)[0])\n",
    "\n",
    "    rnd_decks = pd.Series(rnd_decks, index=df.index)\n",
    "    df['Deck'] = df['Deck'].fillna(rnd_decks)\n",
    "    # df['Deck'] = df['Deck'].astype(np.int8)\n",
    "\n",
    "\n",
    "    df = df.merge(df[df['Group_size']>1][['Side', 'GroupId']].groupby('GroupId')['Side'].agg(pd.Series.mode), how = 'left', left_on= 'GroupId', right_on= 'GroupId', suffixes= ('','_'))\n",
    "    df['Side_'] = Extract(list(df['Side_']))\n",
    "    df['Side'] = df['Side'].fillna(df.Side_)\n",
    "\n",
    "    # Single passengers are filled with random\n",
    "\n",
    "    categories = (df.Side.value_counts() / len(df)).index.to_list()\n",
    "    weights = (df.Side.value_counts() / len(df)).to_list()\n",
    "    rnd_sides = []\n",
    "    for i in range(len(df)):\n",
    "        rnd_sides.append(rnd.choices(categories, weights=weights)[0])\n",
    "\n",
    "    rnd_sides = pd.Series(rnd_sides, index=df.index)\n",
    "    df['Side'] = df['Side'].fillna(rnd_sides)\n",
    "\n",
    "\n",
    " \n",
    "    # Downcasting columns to conserve memory:-    \n",
    "    df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TotalSpend', 'EntSpend', 'LivSpend']] = df[['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TotalSpend', 'EntSpend', 'LivSpend']].astype(np.float32); \n",
    "    \n",
    "    \n",
    "    # Dropping extra columns after usage:-\n",
    "    df = df.drop(['VIP_', 'Num_', 'Deck_', 'Side_'], axis=1, errors= 'ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrtNullHomeDest(df: pd.DataFrame):\n",
    "\n",
    "    def Extract(lst):\n",
    "        new_lst = []\n",
    "        for item in lst:\n",
    "            if ((type(item) != float) and (len(item)!=0)):        # If all are NaNs, then generates string like \"[], Categories (3, object): ['Earth', 'Europa', 'Mars']\"\n",
    "                new_lst.append(item)\n",
    "            else:\n",
    "                new_lst.append(np.nan)\n",
    "        return new_lst \n",
    "\n",
    "\n",
    "    df = df.merge(df[df['Group_size']>1][['HomePlanet', 'GroupId']].groupby('GroupId')['HomePlanet'].agg(pd.Series.mode), how = 'left', left_on= 'GroupId', right_on= 'GroupId', suffixes= ('','_'))\n",
    "    df['HomePlanet_'] = Extract(list(df['HomePlanet_']))\n",
    "    df['HomePlanet'] = df['HomePlanet'].fillna(df.HomePlanet_)\n",
    "\n",
    "    # Single passengers are filled with random by the same distribution\n",
    "    categories = (df.HomePlanet.value_counts() / len(df)).index.to_list()\n",
    "    weights = (df.HomePlanet.value_counts() / len(df)).to_list()\n",
    "    rnd_homes = []\n",
    "    for i in range(len(df)):\n",
    "        rnd_homes.append(rnd.choices(categories, weights=weights)[0])\n",
    "\n",
    "    rnd_homes = pd.Series(rnd_homes, index=df.index)\n",
    "    df['HomePlanet'] = df['HomePlanet'].fillna(rnd_homes)\n",
    "\n",
    "\n",
    "    # Destination looks independent from group, so just fill it with random\n",
    "\n",
    "\n",
    "    # filled with random by the same distribution\n",
    "    categories = (df.Destination.value_counts() / len(df)).index.to_list()\n",
    "    weights = (df.Destination.value_counts() / len(df)).to_list()\n",
    "    rnd_dest = []\n",
    "    for i in range(len(df)):\n",
    "        rnd_dest.append(rnd.choices(categories, weights=weights)[0])\n",
    "\n",
    "    rnd_dest = pd.Series(rnd_dest, index=df.index)\n",
    "    df['Destination'] = df['Destination'].fillna(rnd_dest)\n",
    "\n",
    "\n",
    "    df = df.drop('HomePlanet_', axis=1, errors= 'ignore')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrtNullNumbers(df: pd.DataFrame):\n",
    "    df['Number_of_cabinmates'] = df['Number_of_cabinmates'].fillna(1)\n",
    "    df['Number_of_namesakes'] = df['Number_of_namesakes'].fillna(1)\n",
    "\n",
    "    # Drop features which are not needed\n",
    "    df.drop(['Cabin', 'Name', 'First_name', 'Last_name'], axis=1, inplace=True)\n",
    "    # Cast types and put Transported to the end\n",
    "    df = df.astype({'Age': 'int8', 'GroupId': 'int16', 'Group_size': 'int8', 'PassengerId': 'int8', 'Number_of_cabinmates': 'int8', 'Number_of_namesakes': 'int8'})\n",
    "\n",
    "    if (\"Transported\" in df.columns):\n",
    "        # Outcome to the end\n",
    "        transported = df.pop('Transported')\n",
    "        df['Transported'] = transported\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformCategories(df: pd.DataFrame):\n",
    "    enc = OneHotEncoder(drop='if_binary')\n",
    "    enc_df = pd.DataFrame(enc.fit_transform(df[['HomePlanet', 'Destination', 'Deck', 'Side']]).toarray())\n",
    "\n",
    "    original_labels = df.columns.to_list()\n",
    "    home_labels = ['HP_' + cat for cat in df['HomePlanet'].cat.categories.to_list()]\n",
    "    dest_labels = ['Dest_' + cat for cat in df['Destination'].cat.categories.to_list()]\n",
    "    deck_labels = ['Deck_' + cat for cat in df['Deck'].cat.categories.to_list()]\n",
    "    side_labels = ['Side_S']\n",
    "\n",
    "    new_labels = home_labels + dest_labels + deck_labels + side_labels\n",
    "    complete_new_labels = original_labels + new_labels\n",
    "\n",
    "    df = df.join(enc_df)\n",
    "    df.columns = complete_new_labels\n",
    "\n",
    "    # reduce df size by downcasting types\n",
    "    df[new_labels] = df[new_labels].astype(np.int8)\n",
    "\n",
    "    df.drop(['HomePlanet', 'Destination', 'Deck', 'Side'], axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformBillingLog(df: pd.DataFrame):\n",
    "\n",
    "    billing_columns = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TotalSpend', 'EntSpend', 'LivSpend']\n",
    "\n",
    "    log_trans = PowerTransformer()\n",
    "    transf_df = pd.DataFrame(log_trans.fit_transform(df[billing_columns]))\n",
    "\n",
    "    #df = df.join()\n",
    "\n",
    "    df[billing_columns] = transf_df\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformBillingBins(df: pd.DataFrame):\n",
    "    billing_columns = ['RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TotalSpend', 'EntSpend', 'LivSpend']\n",
    "\n",
    "    # Due to unusual distribution apply custom binning\n",
    "    transf_df = df[billing_columns].copy()\n",
    "    for column in billing_columns:\n",
    "        ninety_quantile = df[column].quantile(q=0.9)\n",
    "        transf_df.loc[(df[column] == 0), column] = 0\n",
    "        transf_df.loc[((df[column] > 0) & (df[column] <= ninety_quantile)), column] = 1\n",
    "        transf_df.loc[(df[column] > ninety_quantile), column] = 2\n",
    "\n",
    "    df[billing_columns] = transf_df\n",
    "\n",
    "    df[billing_columns] = df[billing_columns].astype(np.int8)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumericScale(df: pd.DataFrame):\n",
    "    num_columns = ['Age', 'RoomService','FoodCourt','ShoppingMall','Spa','VRDeck','TotalSpend', 'EntSpend', 'LivSpend', 'GroupId', 'PassengerId', 'Group_size', 'Number_of_cabinmates', 'Num', 'Number_of_namesakes']\n",
    "\n",
    "    sc = StandardScaler()\n",
    "    transf_df = pd.DataFrame(sc.fit_transform(df[num_columns]))\n",
    "\n",
    "    df[num_columns] = transf_df\n",
    "\n",
    "    # Put Transported to the end, as it is the last tranformation\n",
    "    if (\"Transported\" in df.columns):\n",
    "        # Outcome to the end\n",
    "        transported = df.pop('Transported')\n",
    "        df['Transported'] = transported\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompleteTransform(df: pd.DataFrame, binning=False):\n",
    "    if binning:\n",
    "        return NumericScale(TransformBillingBins(TransformCategories(TrtNullNumbers(TrtNullHomeDest(TrtNullVIPCabin(TrtNullAgeCrSlp(add_spending_features(extract_data(df)))))))))\n",
    "    else:\n",
    "        return NumericScale(TransformBillingLog(TransformCategories(TrtNullNumbers(TrtNullHomeDest(TrtNullVIPCabin(TrtNullAgeCrSlp(add_spending_features(extract_data(df)))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaceship-titanic.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "df, df_test, subm_example = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real life we don't have data for prediction before they came. We don't really know distribution of real world data. Therefore, we should fit all transformations only on train set and apply transformation to test set like we don't know it's distribution.\n",
    "\n",
    "In Kaggle competions, sometimes, we have access to test data. In this case, we have complete list of passengers onboard in advance and don't expect new passengers appear. So, we can combine train and test set and transform them together. Then split, train model on train set and perform prediction on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_fake_outcome = pd.merge(df_test, subm_example)\n",
    "tst_ids = df_test_fake_outcome['PassengerId']\n",
    "\n",
    "complete_df = pd.concat([df, df_test_fake_outcome])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform completely\n",
    "complete_df_log = CompleteTransform(complete_df)\n",
    "complete_df_bin = CompleteTransform(complete_df, binning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_log = complete_df_log[~complete_df_log['Full_Id'].isin(list(tst_ids))].drop('Full_Id', axis=1)\n",
    "df_test_log = complete_df_log[complete_df_log['Full_Id'].isin(list(tst_ids))].drop(['Transported', 'Full_Id'], axis=1)\n",
    "\n",
    "df_train_bin = complete_df_bin[~complete_df_bin['Full_Id'].isin(list(tst_ids))].drop('Full_Id', axis=1)\n",
    "df_test_bin = complete_df_bin[complete_df_bin['Full_Id'].isin(list(tst_ids))].drop(['Transported', 'Full_Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_log.to_csv('.//data//prepared_train_log.csv', index=False)\n",
    "df_test_log.to_csv('.//data//prepared_test_log.csv', index=False)\n",
    "df_train_bin.to_csv('.//data//prepared_train_bin.csv', index=False)\n",
    "df_test_bin.to_csv('.//data//prepared_test_bin.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63c2c28c6c61b6d97fedb05da57158fb92cc211e42ce774c010e1a11ba97ef19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
